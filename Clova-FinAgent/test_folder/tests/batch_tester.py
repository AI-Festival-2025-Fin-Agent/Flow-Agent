#!/usr/bin/env python3
"""
í†µí•© ë°°ì¹˜ í…ŒìŠ¤í„°
ëª¨ë“  ìœ í˜•ì˜ ì¿¼ë¦¬(simple, conditional, signal)ë¥¼ ì§€ì›í•˜ëŠ” í†µí•© ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ë„êµ¬
"""

import os
import sys
import json
import csv
import pandas as pd
import argparse
from datetime import datetime
from typing import Dict, List, Any, Optional
import traceback

# Add project root to Python path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from agents.stock_search_agent import StockSearchAgent
from core.database_manager import DatabaseManager


class BatchTester:
    """í†µí•© ë°°ì¹˜ í…ŒìŠ¤í„° í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.db_manager = DatabaseManager(
            company_csv_path=os.path.join(project_root, "company_info.csv"),
            stock_db_path=os.path.join(project_root, "stock_info.db"), 
            market_db_path=os.path.join(project_root, "market_index.db"),
            technical_db_path=os.path.join(project_root, "technical_indicators.db")
        )
        self.agent = StockSearchAgent(self.db_manager)
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.results_dir = os.path.join(project_root, "test_results")
        os.makedirs(self.results_dir, exist_ok=True)
        
        # í…ŒìŠ¤íŠ¸ ì‹œì‘ ì‹œê°„
        self.test_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
    def load_queries(self, query_types: Optional[List[str]] = None) -> List[Dict[str, Any]]:
        """
        ì¿¼ë¦¬ íŒŒì¼ë“¤ì—ì„œ ì§ˆë¬¸ ëª©ë¡ ë¡œë“œ
        
        Args:
            query_types: ë¡œë“œí•  ì¿¼ë¦¬ ìœ í˜• ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  ìœ í˜•)
                        ì˜ˆ: ["simple"], ["conditional", "signal"], ["simple", "conditional", "signal", "open"]
        """
        if query_types is None:
            query_types = ["simple", "conditional", "signal", "open"]
            
        all_queries = []
        
        # CSV íŒŒì¼ë“¤ ë¡œë“œ
        csv_files = [
            ("simple_queries.csv", "simple"),
            ("conditional_queries.csv", "conditional"), 
            ("signal_queries.csv", "signal"),
            ("open_queries.csv", "open")
        ]
        
        for filename, query_type in csv_files:
            if query_type not in query_types:
                continue
                
            filepath = os.path.join(project_root, "query", filename)
            if os.path.exists(filepath):
                try:
                    df = pd.read_csv(filepath)
                    for _, row in df.iterrows():
                        all_queries.append({
                            "type": query_type,
                            "subtype": row.get("type", ""),
                            "evaluation_type": row.get("evaluation_type", "PASS_OR_FAIL"),
                            "question": row["question"],
                            "expected_answer": row["expected_answer"],
                            "source_file": filename
                        })
                    print(f"âœ“ {filename}ì—ì„œ {len(df)}ê°œ ì¿¼ë¦¬ ë¡œë“œ")
                except Exception as e:
                    print(f"âœ— CSV íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ {filename}: {e}")
        
        # JSON íŒŒì¼ë“¤ ë¡œë“œ
        json_files = [
            ("simple_queries_2.json", "simple"),
            ("conditional_queries_2.json", "conditional"),
            ("signal_queries_2.json", "signal"),
            ("open_queries.json", "open")
        ]
        
        for filename, query_type in json_files:
            if query_type not in query_types:
                continue
                
            filepath = os.path.join(project_root, "query", filename)
            if os.path.exists(filepath):
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    for item in data:
                        all_queries.append({
                            "type": query_type,
                            "subtype": "",
                            "evaluation_type": item.get("evaluation_type", "PASS_OR_FAIL"),
                            "question": item["input_data"]["message"],
                            "expected_answer": item["expected_output"],
                            "source_file": filename
                        })
                    print(f"âœ“ {filename}ì—ì„œ {len(data)}ê°œ ì¿¼ë¦¬ ë¡œë“œ")
                except Exception as e:
                    print(f"âœ— JSON íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ {filename}: {e}")
        
        print(f"\nì´ {len(all_queries)}ê°œ ì¿¼ë¦¬ ë¡œë“œ ì™„ë£Œ")
        return all_queries
    
    def test_single_query(self, query_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        question = query_data["question"]
        expected = query_data["expected_answer"]
        
        print(f"\ní…ŒìŠ¤íŠ¸ ì¤‘: {question[:50]}...")
        
        start_time = datetime.now()
        
        try:
            # ì—ì´ì „íŠ¸ ì‹¤í–‰ (ìƒì„¸ ì •ë³´ í¬í•¨)
            detailed_result = self.agent.search(question, return_detailed_info=True)
            
            end_time = datetime.now()
            response_time = (end_time - start_time).total_seconds()
            
            # ìƒì„¸ ì •ë³´ê°€ Dictì¸ì§€ í™•ì¸
            if isinstance(detailed_result, dict):
                actual_result = detailed_result.get("final_result", "")
                agent_detailed_info = {
                    "execution_log": detailed_result.get("execution_log", []),
                    "tool_results": detailed_result.get("tool_results", []),
                    "node_traces": detailed_result.get("node_traces", []),
                    "state_history": detailed_result.get("state_history", []),
                    "final_state": detailed_result.get("final_state", {})
                }
            else:
                # fallback: ë‹¨ìˆœ ë¬¸ìì—´ ê²°ê³¼
                actual_result = str(detailed_result)
                agent_detailed_info = {
                    "execution_log": [],
                    "tool_results": [],
                    "node_traces": [],
                    "state_history": [],
                    "final_state": {}
                }
            
            # ê²°ê³¼ ì €ì¥
            result = {
                "timestamp": start_time.isoformat(),
                "query_type": query_data["type"],
                "subtype": query_data.get("subtype", ""),
                "evaluation_type": query_data.get("evaluation_type", "PASS_OR_FAIL"),
                "question": question,
                "expected_answer": expected,
                "actual_result": actual_result,
                "response_time": response_time,
                "status": "SUCCESS",
                "error_message": None,
                "source_file": query_data.get("source_file", ""),
                # ìƒˆë¡œìš´ ìƒì„¸ ì •ë³´ í•„ë“œë“¤
                "agent_detailed_info": agent_detailed_info,
                "execution_summary": {
                    "total_execution_logs": len(agent_detailed_info["execution_log"]),
                    "total_tool_calls": len(agent_detailed_info["tool_results"]),
                    "total_node_traces": len(agent_detailed_info["node_traces"]),
                    "total_state_changes": len(agent_detailed_info["state_history"]),
                    "final_iterations": agent_detailed_info["final_state"].get("iterations", 0),
                    "final_validation_status": agent_detailed_info["final_state"].get("validation_status", "unknown")
                }
            }
            
            print(f"âœ“ ì™„ë£Œ ({response_time:.2f}ì´ˆ)")
            print(f"  - ì‹¤í–‰ ë¡œê·¸: {len(agent_detailed_info['execution_log'])}ê°œ")
            print(f"  - ë„êµ¬ í˜¸ì¶œ: {len(agent_detailed_info['tool_results'])}ê°œ")
            print(f"  - ë…¸ë“œ ì¶”ì : {len(agent_detailed_info['node_traces'])}ê°œ")
            
            return result
            
        except Exception as e:
            end_time = datetime.now()
            response_time = (end_time - start_time).total_seconds()
            
            error_msg = str(e)
            print(f"âœ— ì˜¤ë¥˜: {error_msg}")
            
            result = {
                "timestamp": start_time.isoformat(),
                "query_type": query_data["type"],
                "subtype": query_data.get("subtype", ""),
                "evaluation_type": query_data.get("evaluation_type", "PASS_OR_FAIL"),
                "question": question,
                "expected_answer": expected,
                "actual_result": f"ERROR: {error_msg}",
                "response_time": response_time,
                "status": "ERROR",
                "error_message": error_msg,
                "source_file": query_data.get("source_file", ""),
                # ì˜¤ë¥˜ ì‹œì—ë„ ë¹ˆ ìƒì„¸ ì •ë³´ êµ¬ì¡° ì œê³µ
                "agent_detailed_info": {
                    "execution_log": [],
                    "tool_results": [],
                    "node_traces": [],
                    "state_history": [],
                    "final_state": {"error": True}
                },
                "execution_summary": {
                    "total_execution_logs": 0,
                    "total_tool_calls": 0,
                    "total_node_traces": 0,
                    "total_state_changes": 0,
                    "final_iterations": 0,
                    "final_validation_status": "error"
                }
            }
            
            return result
    
    def run_batch_test(self, query_types: Optional[List[str]] = None, limit: Optional[int] = None) -> Dict[str, Any]:
        """
        ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        
        Args:
            query_types: í…ŒìŠ¤íŠ¸í•  ì¿¼ë¦¬ ìœ í˜• (Noneì´ë©´ ëª¨ë“  ìœ í˜•)
            limit: í…ŒìŠ¤íŠ¸í•  ìµœëŒ€ ì¿¼ë¦¬ ìˆ˜ (Noneì´ë©´ ëª¨ë“  ì¿¼ë¦¬)
        """
        print("=" * 60)
        print("í†µí•© ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 60)
        
        # ì¿¼ë¦¬ ë¡œë“œ
        queries = self.load_queries(query_types)
        
        if limit:
            queries = queries[:limit]
            print(f"í…ŒìŠ¤íŠ¸ ì œí•œ: {limit}ê°œ ì¿¼ë¦¬ë§Œ ì‹¤í–‰")
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = []
        successful_tests = 0
        failed_tests = 0
        
        for i, query_data in enumerate(queries, 1):
            print(f"\n[{i}/{len(queries)}]", end=" ")
            
            result = self.test_single_query(query_data)
            results.append(result)
            
            if result["status"] == "SUCCESS":
                successful_tests += 1
            else:  
                failed_tests += 1
        
        # ê²°ê³¼ ì €ì¥ (ìƒì„¸ ë¶„ì„ í¬í•¨)
        self._save_results(results, query_types, limit)
        
        # ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_detailed_analysis(results)
        
        # ìš”ì•½ í†µê³„
        summary = {
            "total_queries": len(queries),
            "successful_tests": successful_tests,
            "failed_tests": failed_tests,
            "success_rate": successful_tests / len(queries) * 100 if queries else 0,
            "test_timestamp": self.test_timestamp,
            "query_types_tested": query_types or ["simple", "conditional", "signal"],
            "limit_applied": limit
        }
        
        self._print_summary(summary)
        return summary
    
    def _save_results(self, results: List[Dict[str, Any]], query_types: Optional[List[str]], limit: Optional[int]):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥"""
        
        # ìƒì„¸ ê²°ê³¼ ì €ì¥ (JSON)
        detail_filename = f"test_detailed_{self.test_timestamp}.json"
        detail_path = os.path.join(self.results_dir, detail_filename)
        
        with open(detail_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ìš”ì•½ ê²°ê³¼ ì €ì¥ (CSV)
        summary_filename = f"test_summary_{self.test_timestamp}.csv"
        summary_path = os.path.join(self.results_dir, summary_filename)
        
        df = pd.DataFrame(results)
        df.to_csv(summary_path, index=False, encoding='utf-8-sig')
        
        # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ë§Œ ì €ì¥
        failed_results = [r for r in results if r["status"] == "ERROR"]
        if failed_results:
            failed_filename = f"failed_queries_{self.test_timestamp}.json"
            failed_path = os.path.join(self.results_dir, failed_filename)
            
            with open(failed_path, 'w', encoding='utf-8') as f:
                json.dump(failed_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        print(f"   - ìƒì„¸: {detail_filename} (agent ë‚´ë¶€ ì •ë³´ í¬í•¨)")
        print(f"   - ìš”ì•½: {summary_filename}")
        if failed_results:
            print(f"   - ì‹¤íŒ¨: {failed_filename}")
        print(f"   â€» JSON íŒŒì¼ì— agent ìƒíƒœ, tool ì‹¤í–‰ ì„¸ë¶€ì‚¬í•­, node ì¶”ì  ì •ë³´ê°€ ëª¨ë‘ í¬í•¨ë¨")
    
    def _print_summary(self, summary: Dict[str, Any]):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        print(f"ì „ì²´ ì¿¼ë¦¬ ìˆ˜: {summary['total_queries']}")
        print(f"ì„±ê³µ: {summary['successful_tests']}")
        print(f"ì‹¤íŒ¨: {summary['failed_tests']}")
        print(f"ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
        print(f"í…ŒìŠ¤íŠ¸ ìœ í˜•: {', '.join(summary['query_types_tested'])}")
        if summary['limit_applied']:
            print(f"ì œí•œ ì ìš©: {summary['limit_applied']}ê°œ")
        print("=" * 60)
    
    def _generate_detailed_analysis(self, results: List[Dict[str, Any]]):
        """ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not results:
            return
            
        print(f"\nğŸ“Š ìƒì„¸ ì‹¤í–‰ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        # ë„êµ¬ ì‚¬ìš© í†µê³„
        tool_usage_stats = {}
        tool_performance_stats = {}
        node_execution_stats = {}
        validation_status_stats = {}
        
        for result in results:
            detailed_info = result.get("agent_detailed_info", {})
            
            # ë„êµ¬ ì‚¬ìš© í†µê³„ ìˆ˜ì§‘
            for tool_result in detailed_info.get("tool_results", []):
                tool_name = tool_result.get("tool_name", "unknown")
                
                if tool_name not in tool_usage_stats:
                    tool_usage_stats[tool_name] = {
                        "count": 0,
                        "success_count": 0,
                        "param_error_count": 0,
                        "error_count": 0,
                        "total_execution_time": 0.0,
                        "avg_execution_time": 0.0
                    }
                
                tool_usage_stats[tool_name]["count"] += 1
                
                status = tool_result.get("status", "unknown")
                if status == "success":
                    tool_usage_stats[tool_name]["success_count"] += 1
                elif status == "param_missing":
                    tool_usage_stats[tool_name]["param_error_count"] += 1
                else:
                    tool_usage_stats[tool_name]["error_count"] += 1
                
                exec_time = tool_result.get("execution_time", 0.0)
                tool_usage_stats[tool_name]["total_execution_time"] += exec_time
            
            # ë…¸ë“œ ì‹¤í–‰ í†µê³„
            for node_trace in detailed_info.get("node_traces", []):
                node_name = node_trace.get("node_name", "unknown")
                if node_name not in node_execution_stats:
                    node_execution_stats[node_name] = 0
                node_execution_stats[node_name] += 1
            
            # ê²€ì¦ ìƒíƒœ í†µê³„
            final_status = detailed_info.get("final_state", {}).get("validation_status", "unknown")
            if final_status not in validation_status_stats:
                validation_status_stats[final_status] = 0
            validation_status_stats[final_status] += 1
        
        # í‰ê·  ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
        for tool_name, stats in tool_usage_stats.items():
            if stats["count"] > 0:
                stats["avg_execution_time"] = stats["total_execution_time"] / stats["count"]
        
        # ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥
        analysis_report = {
            "generated_at": datetime.now().isoformat(),
            "total_tests": len(results),
            "tool_usage_statistics": tool_usage_stats,
            "node_execution_statistics": node_execution_stats,
            "validation_status_statistics": validation_status_stats,
            "performance_summary": {
                "most_used_tools": sorted(tool_usage_stats.items(), key=lambda x: x[1]["count"], reverse=True)[:5],
                "slowest_tools": sorted(tool_usage_stats.items(), key=lambda x: x[1]["avg_execution_time"], reverse=True)[:5],
                "most_error_prone_tools": sorted(
                    [(name, stats) for name, stats in tool_usage_stats.items() if stats["count"] > 0],
                    key=lambda x: (x[1]["error_count"] + x[1]["param_error_count"]) / x[1]["count"],
                    reverse=True
                )[:5]
            }
        }
        
        # ë¶„ì„ ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
        analysis_filename = f"detailed_analysis_{self.test_timestamp}.json"
        analysis_path = os.path.join(self.results_dir, analysis_filename)
        
        with open(analysis_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥: {analysis_filename}")
        
        # ì½˜ì†”ì— ì£¼ìš” í†µê³„ ì¶œë ¥
        print(f"\nğŸ” ì£¼ìš” í†µê³„:")
        print(f"  - ì´ ë„êµ¬ í˜¸ì¶œ: {sum(stats['count'] for stats in tool_usage_stats.values())}")
        print(f"  - ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ë„êµ¬: {analysis_report['performance_summary']['most_used_tools'][0][0] if analysis_report['performance_summary']['most_used_tools'] else 'N/A'}")
        print(f"  - í‰ê·  ì‹¤í–‰ ì‹œê°„ì´ ê°€ì¥ ê¸´ ë„êµ¬: {analysis_report['performance_summary']['slowest_tools'][0][0] if analysis_report['performance_summary']['slowest_tools'] else 'N/A'}")
        
        if validation_status_stats:
            print(f"  - ê²€ì¦ ìƒíƒœ ë¶„í¬:")
            for status, count in validation_status_stats.items():
                print(f"    * {status}: {count}ê°œ ({count/len(results)*100:.1f}%)")
        
        print("=" * 60)


def parse_arguments():
    """CLI ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description="í†µí•© ë°°ì¹˜ í…ŒìŠ¤í„° - ì£¼ì‹ ê²€ìƒ‰ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python batch_tester.py                                    # ëŒ€í™”í˜• ëª¨ë“œ
  python batch_tester.py --limit 10                         # ì²˜ìŒ 10ê°œë§Œ í…ŒìŠ¤íŠ¸
  python batch_tester.py --type simple                      # ê¸°ë³¸ ì¿¼ë¦¬ë§Œ í…ŒìŠ¤íŠ¸
  python batch_tester.py --type conditional --limit 5       # ì¡°ê±´ ì¿¼ë¦¬ 5ê°œë§Œ í…ŒìŠ¤íŠ¸
  python batch_tester.py --type simple,signal               # ê¸°ë³¸+ì‹œê·¸ë„ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
  python batch_tester.py --all                              # ëª¨ë“  ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ (ëŒ€í™”í˜• ì—†ì´)
        """
    )
    
    parser.add_argument(
        "--type", 
        type=str,
        help="í…ŒìŠ¤íŠ¸í•  ì¿¼ë¦¬ ìœ í˜• (simple, conditional, signal, open). ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì—¬ëŸ¬ ê°œ ì§€ì • ê°€ëŠ¥"
    )
    
    parser.add_argument(
        "--limit",
        type=int,
        help="í…ŒìŠ¤íŠ¸í•  ìµœëŒ€ ì¿¼ë¦¬ ìˆ˜"
    )
    
    parser.add_argument(
        "--all",
        action="store_true",
        help="ëª¨ë“  ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ (ëŒ€í™”í˜• ëª¨ë“œ ê±´ë„ˆë›°ê¸°)"
    )
    
    parser.add_argument(
        "--interactive",
        action="store_true",
        help="ëŒ€í™”í˜• ëª¨ë“œ ê°•ì œ ì‹¤í–‰ (ë‹¤ë¥¸ ì˜µì…˜ ë¬´ì‹œ)"
    )
    
    return parser.parse_args()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    args = parse_arguments()
    
    print("í†µí•© ë°°ì¹˜ í…ŒìŠ¤í„°")
    print("=" * 50)
    
    tester = BatchTester()
    
    # ëŒ€í™”í˜• ëª¨ë“œ ê°•ì œ ì‹¤í–‰
    if args.interactive:
        run_interactive_mode(tester)
        return
    
    # CLI ì¸ìê°€ ìˆìœ¼ë©´ ë°”ë¡œ ì‹¤í–‰
    if args.type or args.limit or args.all:
        run_cli_mode(tester, args)
        return
    
    # ì¸ìê°€ ì—†ìœ¼ë©´ ëŒ€í™”í˜• ëª¨ë“œ
    run_interactive_mode(tester)

def run_cli_mode(tester: BatchTester, args):
    """CLI ëª¨ë“œ ì‹¤í–‰"""
    try:
        # ì¿¼ë¦¬ ìœ í˜• íŒŒì‹±
        query_types = None
        if args.type:
            query_types = [t.strip() for t in args.type.split(",")]
            # ìœ íš¨ì„± ê²€ì‚¬
            valid_types = ["simple", "conditional", "signal", "open"]
            for qt in query_types:
                if qt not in valid_types:
                    print(f"âŒ ì˜ëª»ëœ ì¿¼ë¦¬ ìœ í˜•: {qt}")
                    print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ìœ í˜•: {', '.join(valid_types)}")
                    return
        elif args.all:
            query_types = ["simple", "conditional", "signal", "open"]
        
        # í…ŒìŠ¤íŠ¸ ì •ë³´ ì¶œë ¥
        type_str = f" ({', '.join(query_types)})" if query_types else ""
        limit_str = f" (ìµœëŒ€ {args.limit}ê°œ)" if args.limit else ""
        print(f"\nğŸš€ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹œì‘{type_str}{limit_str}")
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        summary = tester.run_batch_test(query_types=query_types, limit=args.limit)
        
        print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()

def run_interactive_mode(tester: BatchTester):
    """ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰"""
    print("\ní…ŒìŠ¤íŠ¸ ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ì „ì²´ í…ŒìŠ¤íŠ¸ (ëª¨ë“  ìœ í˜•, ëª¨ë“  ì¿¼ë¦¬)")
    print("2. ê¸°ë³¸ ì¿¼ë¦¬ë§Œ í…ŒìŠ¤íŠ¸")
    print("3. ì¡°ê±´ ì¿¼ë¦¬ë§Œ í…ŒìŠ¤íŠ¸") 
    print("4. ì‹œê·¸ë„ ì¿¼ë¦¬ë§Œ í…ŒìŠ¤íŠ¸")
    print("5. ì˜¤í”ˆ ì¿¼ë¦¬ë§Œ í…ŒìŠ¤íŠ¸")
    print("6. ì œí•œëœ í…ŒìŠ¤íŠ¸ (ì²˜ìŒ 50ê°œ)")
    print("7. ì œí•œëœ í…ŒìŠ¤íŠ¸ (ì²˜ìŒ 5ê°œ)")
    
    choice = input("\nì„ íƒ (1-6): ").strip()
    
    try:
        if choice == "1":
            print("\nì „ì²´ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            summary = tester.run_batch_test()
            
        elif choice == "2":
            print("\nê¸°ë³¸ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            summary = tester.run_batch_test(query_types=["simple"])
            
        elif choice == "3":
            print("\nì¡°ê±´ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            summary = tester.run_batch_test(query_types=["conditional"])
            
        elif choice == "4":
            print("\nì‹œê·¸ë„ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            summary = tester.run_batch_test(query_types=["signal"])
            
        elif choice == "5":
            print("\nì˜¤í”ˆ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            summary = tester.run_batch_test(query_types=["open"])
            
        elif choice == "6":
            print("\nì œí•œëœ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ (50ê°œ)ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            summary = tester.run_batch_test(limit=50)
            
        elif choice == "7":
            print("\nì œí•œëœ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ (5ê°œ)ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            summary = tester.run_batch_test(limit=5)
            
        else:
            print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
            return
            
        print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    main()